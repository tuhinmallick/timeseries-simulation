# Import for excel downloading
import base64
import webbrowser
from datetime import timedelta
from io import BytesIO

# Dataframe standard packages
import numpy as np
import pandas as pd
import streamlit as st
from streamlit import caching

# User settings
import settings

# Project packages
import wsb_reasoner

# Config streamlit page
# st.set_page_config(
#     page_title="Reddit Sentiment Analysis",
#     page_icon="ðŸš€",
#     initial_sidebar_state="expanded",
#     layout='wide'
# )

# Sidebar
# st.sidebar.image(r'wsb_face.png', use_column_width='auto')


# Start page
def reedit_sentiments():
    # Get the last time (to check for updates in the db)
    last_updated_utc = wsb_reasoner.get_last_upload_time()

    @st.cache(max_entries=5)
    def get_last_data_from_db(last_updated):

        # Get latest data
        comments_and_scores = wsb_reasoner.get_comments_and_scores_from_db()
        comments_and_scores = (
            comments_and_scores.explode(column="tickers")
            .reset_index(drop=True)
            .rename(columns={"tickers": "ticker"})
        )

        return comments_and_scores

    def get_top_frames(
        vader_compare, cut_off_now, cut_off_before, top=settings.top_elements
    ):
        # Copy dataframe to change by value
        vader_compare = vader_compare.copy()

        # Cut-off times
        cut_off_now = pd.to_datetime(cut_off_now)
        cut_off_before = pd.to_datetime(cut_off_before)

        # Get dfs and merge in a single frame for comparison
        vader_1 = (
            vader_compare[vader_compare.created_utc >= cut_off_now]
            .groupby("ticker")
            .agg({"ticker": "count", "compound": "mean"})
            .rename(columns={"ticker": "mentions", "compound": "mean_sentiment"})
        )
        print("Groupby 1")
        print(vader_1)
        vader_1 = vader_1.sort_values("mentions", ascending=False)

        print(vader_1)
        vader_1 = vader_1.add_suffix("_T1")
        print(vader_1)

        vader_0 = (
            vader_compare[
                (vader_compare.created_utc >= cut_off_before)
                & (vader_compare.created_utc < cut_off_now)
            ]
            .groupby("ticker")
            .agg({"ticker": "count", "compound": "mean"})
            .rename(columns={"ticker": "mentions", "compound": "mean_sentiment"})
        )
        print(vader_0)
        vader_0 = vader_0.sort_values("mentions", ascending=False)

        vader_0 = vader_0.add_suffix("_T0")

        vader_compare = vader_1.merge(
            vader_0, how="left", left_index=True, right_index=True
        )
        vader_compare.mean_sentiment_T0 = round(vader_compare.mean_sentiment_T0, 4)
        vader_compare.mean_sentiment_T1 = round(vader_compare.mean_sentiment_T1, 4)

        # Make comparisons
        vader_compare["delta_mentions"] = (
            vader_compare.mentions_T1 / vader_compare.mentions_T0 - 1
        )
        vader_compare["delta_score"] = (
            vader_compare.mean_sentiment_T1 - vader_compare.mean_sentiment_T0
        )

        # Prettify columns
        vader_compare = (
            vader_compare[
                ["mentions_T1", "delta_mentions", "mean_sentiment_T1", "delta_score"]
            ]
            .rename(
                columns={
                    "mentions_T1": "Mentions",
                    "delta_mentions": "%Î” Mentions",
                    "mean_sentiment_T1": "Score",
                    "delta_score": "Î” Score",
                }
            )
            .head(top)
        )

        # Format deltas
        def red_green(cell):
            if type(cell) != str and cell < 0:
                return "color: Crimson"
            elif type(cell) != str and cell >= 0:
                return "color: ForestGreen"
            else:
                return "color: black"

        # Format scores and mentions
        def red_blue_green(cell):
            if type(cell) != str and cell <= -0.05:
                return "color: Crimson"
            elif type(cell) != str and cell >= 0.05:
                return "color: ForestGreen"
            if type(cell) != str and -0.05 < cell < 0.05:
                return "color: DarkCyan"
            else:
                return "color: black"

        # Show stylized frame
        t_frame = (
            vader_compare.style.applymap(
                red_green, subset=pd.IndexSlice[:, ["%Î” Mentions", "Î” Score"]]
            )
            .format(
                {"%Î” Mentions": "{:.2%}", "Score": "{:.4}", "Î” Score": "{:.4}"},
                na_rep="New",
            )
            .apply(
                lambda row: np.repeat(
                    "background: lightgrey" if row.isnull().any() else "", row.shape[0]
                ),
                axis=1,
            )
            .applymap(red_blue_green, subset=pd.IndexSlice[:, ["Score"]])
        )

        return t_frame

    # Get data
    data = get_last_data_from_db(last_updated_utc)

    # Start page
    st.title("Reddit  Sentiment Analysis")

    # Get dates
    min_date = (data.created_utc.min() + timedelta(hours=24)).to_pydatetime().date()
    max_date = (data.created_utc.max() - timedelta(hours=24)).to_pydatetime().date()

    # Ask for dates input
    date = st.sidebar.date_input(
        "Select day for top list preview:",
        min_value=min_date,
        max_value=max_date,
        value=max_date,
    )

    st.write("### Tickers insights")

    # Add ticker selector to sidebar
    ticker_to_show = st.sidebar.selectbox(
        label="Set ticker:", options=tuple(data.ticker.value_counts().keys().to_list())
    )
    st.sidebar.write(f"Number of tickers available : {data.shape[0]}")
    # Add slider for time frames to use for aggregation
    frequency = st.sidebar.slider(
        label="Set frequency for aggregation of scores and mentions:",
        min_value=0.25,
        max_value=6.0,
        value=0.75,
        step=0.25,
        format="%f hours",
        help=" interval applies for both graphs below",
    )
    mentions_exp = st.expander("Mentions")
    mentions_exp.subheader("Mentions by time for $" + ticker_to_show)

    # Get charts data with scores reindex by frequency
    chart_data_main = data[data.ticker == ticker_to_show].copy()
    chart_data_main["Mentions"] = 1
    chart_data_main = (
        chart_data_main.groupby(
            [pd.Grouper(freq=str(int(frequency * 60)) + "min", key="created_utc")]
        )[["neg", "neu", "pos", "compound", "Mentions"]]
        .agg(
            {
                "neg": "mean",
                "neu": "mean",
                "pos": "mean",
                "compound": "mean",
                "Mentions": "count",
            }
        )
        .rename(
            columns={
                "neg": "Negative",
                "neu": "Neutral",
                "pos": "Positive",
                "compound": "Compound",
            }
        )
    )
    chart_data_main.reindex(
        pd.date_range(
            chart_data_main.index.min(),
            chart_data_main.index.max(),
            freq=str(int(frequency * 60)) + "min",
        )
    )
    chart_data_main = chart_data_main.fillna(0)

    # Mentions graph
    chart_data = chart_data_main[["Mentions"]]
    mentions_exp.line_chart(chart_data)
    sentiments_exp = st.expander("Sentiments")
    sentiments_exp.subheader("Sentiments by time for $" + ticker_to_show)
    sentiment_type = st.sidebar.selectbox(
        "Set sentiment:", ("Compound", "Negative", "Positive", "Neutral")
    )

    # Split rows
    # col1, col2 = st.columns([3, 8])

    # Add slider for look-back period
    look_back = st.sidebar.slider(
        label="Look-back period for moving averages:",
        min_value=6.0,
        max_value=48.0,
        value=8.0,
        step=0.25,
        format="%f hours",
    )
    span = int(look_back / frequency)
    cols_for_chart = st.sidebar.multiselect(
        "Check which lines to plot on graph:",
        options=[
            sentiment_type,
            "EWM (" + str(look_back) + " hours)",
            "EWM (" + str(look_back * 2) + " hours)",
            "SMA (" + str(look_back) + " hours)",
            "SMA (" + str(look_back * 2) + " hours)",
        ],
        default=[
            "EWM (" + str(look_back) + " hours)",
            "SMA (" + str(look_back) + " hours)",
        ],
    )

    # Prepare data for chart
    chart_data = chart_data_main[[sentiment_type]].copy()
    chart_data["EWM (" + str(look_back) + " hours)"] = (
        chart_data[sentiment_type].ewm(span=span).mean()
    )
    chart_data["EWM (" + str(look_back * 2) + " hours)"] = (
        chart_data[sentiment_type].ewm(span=span * 2).mean()
    )
    chart_data["SMA (" + str(look_back) + " hours)"] = (
        chart_data[sentiment_type].rolling(span).mean()
    )
    chart_data["SMA (" + str(look_back * 2) + " hours)"] = (
        chart_data[sentiment_type].rolling(span * 2).mean()
    )
    chart_data = chart_data[cols_for_chart]

    # Chart data
    sentiments_exp.line_chart(chart_data)

    # Top list
    with st.expander("Top " + str(settings.top_elements) + " mentions"):
        st.subheader(
            "Top "
            + str(settings.top_elements)
            + " mentions for  "
            + date.strftime("%A, %B %e, %Y")
        )
        st.table(get_top_frames(data, date, date - timedelta(days=1)))

    explanation = """The score is the mean compound sentiment score for the horizon period.
    Rule of thumb for interpretation: (>0.5) is positive; (<-0.5) is negative; else is neutral.

    VADER (Valence Aware Dictionary and sEntiment Reasoner) analysis tool is applied whose scoring is accustomed to social network text and was tuned to recognize lexicon like smiles and emojis.
    The top list shows top mentions and VADER scores for respective tickers.
    Mentions and scores are compared on daily basis.
    False positives are possible with one-letter tickers like $B and $F.
    The top list and insight tickers data are updated on an hourly basis.
    Tickers list for searching is from ALPHA VANTAGE.
    """
    with st.expander("How to interpret the above table"):
        st.write(explanation)


#     # Comments downloader (removed for sake of RAM performance)
#         # Place link to download comments for particular ticker
#     def df_to_excel_link(df):
#         output = BytesIO()
#         writer = pd.ExcelWriter(output, engine='xlsxwriter')
#         df.to_excel(writer, sheet_name='Sheet1')
#         writer.save()
#         processed_data = output.getvalue()
#         b64 = base64.b64encode(processed_data)

#         return f'<a href="data:application/octet-stream;base64,{b64.decode()}" download="extract.xlsx">Click to ' \
#             f'download comments and vader scores for ${ticker_to_show} in Excel format (Strong language!)</a>'


#         # Download tickers comments with scores data
#         ticker_comments = data[data.ticker == ticker_to_show].copy()
#         ticker_comments = ticker_comments.sort_values(by='created_utc', ascending=False).reset_index(drop=True)
#         st.markdown(df_to_excel_link(ticker_comments), unsafe_allow_html=True)
